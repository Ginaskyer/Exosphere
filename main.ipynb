{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615043bb-3302-4b2c-bad7-2a365fece12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.20\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e44aec9-f4b2-407b-802e-04995f600c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-24 17:59:48--  https://www.exosphere.fuchuanpu.xyz/dataset.zip\n",
      "2606:4700:3030::6815:3001, 2606:4700:3030::6815:5001, 2606:4700:3030::6815:2001, ...\n",
      "connected. to www.exosphere.fuchuanpu.xyz (www.exosphere.fuchuanpu.xyz)|2606:4700:3030::6815:3001|:443... \n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1149984399 (1.1G) [application/zip]\n",
      "Saving to: ‘dataset.zip.1’\n",
      "\n",
      "dataset.zip.1       100%[===================>]   1.07G  24.3MB/s    in 37s     \n",
      "\n",
      "2025-02-24 18:00:27 (29.2 MB/s) - ‘dataset.zip.1’ saved [1149984399/1149984399]\n",
      "\n",
      "Archive:  dataset.zip\n",
      "   creating: dataset/\n",
      "  inflating: dataset/amp_memcached.txt  \n",
      "  inflating: dataset/app_rdp.txt     \n",
      "  inflating: dataset/Whisper_SYNDOS.txt  \n",
      "  inflating: dataset/app_dns.txt     \n",
      "  inflating: dataset/Whisper_SSDPDOS.txt  \n",
      "  inflating: dataset/Whisper_FUZZ.txt  \n",
      "  inflating: dataset/amp_dns.txt     \n",
      "  inflating: dataset/bruteforce_syn.txt  \n",
      "  inflating: dataset/amp_cldap.txt   \n",
      "  inflating: dataset/Whisper_UDPDOS.txt  \n",
      "  inflating: dataset/bruteforce_rst.txt  \n",
      "  inflating: dataset/bruteforce_udp.txt  \n",
      "  inflating: dataset/app_netbios.txt  \n",
      "  inflating: dataset/bruteforce_icmp.txt  \n",
      "  inflating: dataset/app_smtp.txt    \n",
      "  inflating: dataset/amp_ntp.txt     \n",
      "usage: rm [-f | -i] [-dIPRrvWx] file ...\n",
      "       unlink [--] file\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.exosphere.fuchuanpu.xyz/dataset.zip\n",
    "!unzip dataset.zip && rm $_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fe3528-3dd1-47dd-be87-5056cdc512ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common.py\n",
    "\n",
    "import time\n",
    "import functools\n",
    "import socket\n",
    "import struct\n",
    "\n",
    "col_dic = {\"d\": 30, \"r\": 31, \"g\": 32, \"y\": 33, \"b\": 34, \"p\": 35, \"c\": 36, \"w\": 37}\n",
    "\n",
    "\n",
    "def string_decorator(basic: str, /, col: str = \"w\", b: bool = False) -> str:\n",
    "    return f\"\\033[{'1;' if b else '0;'}{col_dic[col]}m{basic}\\033[0m\"\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=string_decorator(\"[%(asctime)s %(levelname)s]\", \"c\") + \" %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "\n",
    "def call_log(text: str = \"\"):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kw):\n",
    "            if len(text) == 0 or text.isspace():\n",
    "                print(string_decorator(f\"[Call Function {func.__name__}()]\", \"g\", True))\n",
    "            else:\n",
    "                print(\n",
    "                    string_decorator(\n",
    "                        f\"[Call Function {func.__name__}()]: {text}\", \"g\", True\n",
    "                    )\n",
    "                )\n",
    "            return func(*args, **kw)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def time_log(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kw):\n",
    "        _time_str = time.time()\n",
    "        res = func(*args, **kw)\n",
    "        _time_stp = time.time()\n",
    "        print(\n",
    "            string_decorator(\n",
    "                f\"[Function {func.__name__}() Finished] cost time: {_time_stp - _time_str:6.3f} s.\",\n",
    "                \"p\",\n",
    "                True,\n",
    "            )\n",
    "        )\n",
    "        return res\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def addr_num2str(num_addr: int) -> str:\n",
    "    return socket.inet_ntoa(struct.pack(\"I\", socket.htonl(num_addr)))\n",
    "\n",
    "\n",
    "def addr_str2num(str_addr: str) -> int:\n",
    "    return socket.ntohl(struct.unpack(\"I\", socket.inet_aton(str(str_addr)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd7e082-be7c-4828-9cfa-b4be77e2e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "T_MAX = 100\n",
    "ETH_MTU = 1500\n",
    "\n",
    "\n",
    "@time_log\n",
    "def load_data(\n",
    "    data_tag: str,\n",
    "    data_target: str,\n",
    "    shuffle_seed=None,\n",
    "    fig_path=None,\n",
    "    train_ratio=0.80,\n",
    "    segment_len=1000,\n",
    "    time_base=1e-6,\n",
    ") -> Tuple[\n",
    "    torch.FloatTensor,\n",
    "    torch.FloatTensor,\n",
    "    torch.FloatTensor,\n",
    "    torch.FloatTensor,\n",
    "    List[List[int]],\n",
    "    List[List[int]],\n",
    "    List[List[int]],\n",
    "    List[List[int]],\n",
    "]:\n",
    "\n",
    "    if not os.path.exists(data_target):\n",
    "        exit(-1)\n",
    "\n",
    "    logging.info(f\"Read dataset from {data_target}\")\n",
    "    tim_vec, len_vec, label_vec = [], [], []\n",
    "    with open(data_target, \"r\") as f:\n",
    "        for ll in f.readlines():\n",
    "            [a, b, c] = list(map(float, ll.split()))\n",
    "            tim_vec.append(math.floor(a * (1 / time_base)))\n",
    "            len_vec.append(min(b, ETH_MTU) / ETH_MTU)\n",
    "            label_vec.append(int(c))\n",
    "\n",
    "    iat_vec = [0]\n",
    "    start = tim_vec[0]\n",
    "    for i in range(1, len(tim_vec)):\n",
    "        iat_vec.append(min(max(tim_vec[i] - start, 0), T_MAX))\n",
    "        start = max(start, tim_vec[i])\n",
    "\n",
    "    feature_vec = []\n",
    "    real_label_vec = []\n",
    "    num_packet_vec = []\n",
    "    num_attack_vec = []\n",
    "    index = 0\n",
    "    while index < len(iat_vec):\n",
    "        time_var = iat_vec[index]\n",
    "        feature_vec.append(-time_var / T_MAX)\n",
    "        real_label_vec.append(0)\n",
    "        num_packet_vec.append(0)\n",
    "        num_attack_vec.append(0)\n",
    "\n",
    "        feature_vec.append(len_vec[index])\n",
    "        num_packet_vec.append(1)\n",
    "        real_label_vec.append(label_vec[index])\n",
    "        num_attack_vec.append(int(label_vec[index]))\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    seg_feature_vec, seg_label_vec = [], []\n",
    "    seg_num_vec, seg_attack_num_vec = [], []\n",
    "    for i in range(0, len(feature_vec), segment_len):\n",
    "        if i + segment_len >= len(feature_vec):\n",
    "            continue\n",
    "        seg_feature_vec.append(feature_vec[i : i + segment_len])\n",
    "        seg_label_vec.append(real_label_vec[i : i + segment_len])\n",
    "        seg_num_vec.append(num_packet_vec[i : i + segment_len])\n",
    "        seg_attack_num_vec.append(num_attack_vec[i : i + segment_len])\n",
    "\n",
    "    if shuffle_seed is not None:\n",
    "        random.seed(shuffle_seed)\n",
    "        _shu_ls = list(\n",
    "            zip(seg_feature_vec, seg_label_vec, seg_num_vec, seg_attack_num_vec)\n",
    "        )\n",
    "        random.shuffle(_shu_ls)\n",
    "        seg_feature_vec, seg_label_vec, seg_num_vec, seg_attack_num_vec = (\n",
    "            [x[0] for x in _shu_ls],\n",
    "            [x[1] for x in _shu_ls],\n",
    "            [x[2] for x in _shu_ls],\n",
    "            [x[3] for x in _shu_ls],\n",
    "        )\n",
    "\n",
    "    train_test_line = int(train_ratio * len(seg_feature_vec))\n",
    "    train_data = seg_feature_vec[:train_test_line]\n",
    "    train_label = seg_label_vec[:train_test_line]\n",
    "    test_data = seg_feature_vec[train_test_line:]\n",
    "    test_label = seg_label_vec[train_test_line:]\n",
    "\n",
    "    train_num = seg_num_vec[:train_test_line]\n",
    "    test_num = seg_num_vec[train_test_line:]\n",
    "    train_atc_num = seg_attack_num_vec[:train_test_line]\n",
    "    test_atc_num = seg_attack_num_vec[train_test_line:]\n",
    "\n",
    "    train_data = torch.FloatTensor(train_data).unsqueeze(1)\n",
    "    train_label = torch.FloatTensor(train_label).unsqueeze(1)\n",
    "    test_data = torch.FloatTensor(test_data).unsqueeze(1)\n",
    "    test_label = torch.FloatTensor(test_label).unsqueeze(1)\n",
    "\n",
    "    logging.info(\n",
    "        f\"[{data_tag}] Attack Frames: {label_vec.count(True)}, Benign Frames: {len(label_vec) - label_vec.count(True)}.\"\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"[{data_tag}] Train Records: {train_label.size(0)}, Test Records: {test_label.size(0)}\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_data,\n",
    "        train_label,\n",
    "        test_data,\n",
    "        test_label,\n",
    "        train_num,\n",
    "        test_num,\n",
    "        train_atc_num,\n",
    "        test_atc_num,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec05b00a-d8ac-4f90-ba5d-2503e20c3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def dice_loss(prediction, target, smooth=1.0):\n",
    "\n",
    "    i_, t_ = prediction.view(-1), target.view(-1)\n",
    "    inter = (i_ * t_).sum()\n",
    "\n",
    "    return 1 - ((2.0 * inter + smooth) / (i_.sum() + t_.sum() + smooth))\n",
    "\n",
    "\n",
    "def calc_loss(prediction, target, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(prediction, target)\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "\n",
    "    dice = dice_loss(prediction, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c7dfc22-b233-4a28-b718-3cd50a28dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Exosphere(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch=3, out_ch=1):\n",
    "        super(Exosphere, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(in_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv1d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        d3 = self.Up3(e3)\n",
    "        d3 = torch.cat((e2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((e1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ce1cff9-ea03-48b7-a8c3-84fec469798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    matthews_corrcoef,\n",
    "    fbeta_score,\n",
    ")\n",
    "\n",
    "\n",
    "mtx = List[List[int]]\n",
    "\n",
    "\n",
    "@time_log\n",
    "def train_test(\n",
    "    data_tag: str,\n",
    "    log_path: str,\n",
    "    fig_path: str,\n",
    "    trainD: torch.FloatTensor,\n",
    "    trainL: torch.FloatTensor,\n",
    "    testD: torch.FloatTensor,\n",
    "    testL: torch.FloatTensor,\n",
    "    trainN: mtx,\n",
    "    testN: mtx,\n",
    "    trainA: mtx,\n",
    "    testA: mtx,\n",
    "    gpu_id: int,\n",
    "    waterline: float,\n",
    "    lr=0.001,\n",
    "    batch_size=4,\n",
    "    num_epoch=15,\n",
    "):\n",
    "\n",
    "    logging.info(f\"[{data_tag}] is started.\")\n",
    "\n",
    "    fout = open(log_path, \"w\", buffering=1)\n",
    "    exosphere = Exosphere(in_ch=1, out_ch=1)\n",
    "\n",
    "    opt = torch.optim.Adam(exosphere.parameters(), lr=lr)\n",
    "\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    print(\n",
    "        (\n",
    "            f\"[{data_tag}] Use GPU: {gpu_id}.\"\n",
    "            if train_on_gpu\n",
    "            else f\"[{data_tag}] Use CPU.\"\n",
    "        ),\n",
    "        file=fout,\n",
    "        flush=True,\n",
    "    )\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if train_on_gpu else \"cpu\")\n",
    "    exosphere.to(device)\n",
    "    trainD.to(device)\n",
    "    trainL.to(device)\n",
    "    testD.to(device)\n",
    "    testL.to(device)\n",
    "\n",
    "    print(\n",
    "        \"Total Parameters:\",\n",
    "        sum([p.nelement() for p in exosphere.parameters()]),\n",
    "        file=fout,\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    for e in range(num_epoch):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        exosphere.train()\n",
    "        num_train = 0\n",
    "        for i in range(0, trainD.size(0), batch_size):\n",
    "            if i + batch_size >= trainD.size(0):\n",
    "                continue\n",
    "\n",
    "            x = trainD[i : i + batch_size].to(device)\n",
    "            y = trainL[i : i + batch_size].to(device)\n",
    "            num_train += batch_size\n",
    "\n",
    "            opt.zero_grad()\n",
    "            y_pred = exosphere(x)\n",
    "            lossT = calc_loss(y_pred, y)\n",
    "\n",
    "            train_loss += lossT.item() * x.size(0)\n",
    "            lossT.backward()\n",
    "            opt.step()\n",
    "\n",
    "        exosphere.eval()\n",
    "        torch.no_grad()\n",
    "\n",
    "        pred_res = []\n",
    "        label_res = []\n",
    "        num_res = []\n",
    "        num_atc_res = []\n",
    "\n",
    "        num_test = 0\n",
    "        sum_test_time = 0\n",
    "\n",
    "        test_batch_size = batch_size * 20\n",
    "        for i in range(0, testD.size(0), test_batch_size):\n",
    "            if i + test_batch_size >= testD.size(0):\n",
    "                continue\n",
    "\n",
    "            x = testD[i : i + test_batch_size].to(device)\n",
    "            y = testL[i : i + test_batch_size].to(device)\n",
    "            num_test += test_batch_size\n",
    "\n",
    "            start_test = time.time()\n",
    "            y_pred = exosphere(x)\n",
    "            end_test = time.time()\n",
    "            sum_test_time += end_test - start_test\n",
    "            lossL = calc_loss(y_pred, y)\n",
    "\n",
    "            test_loss += lossL.item() * x.size(0)\n",
    "            pred_res.extend(y_pred.view(-1).tolist())\n",
    "            label_res.extend(y.view(-1).tolist())\n",
    "\n",
    "            for x in range(i, i + test_batch_size):\n",
    "                num_res.extend(testN[x])\n",
    "                num_atc_res.extend(testA[x])\n",
    "\n",
    "        train_loss /= num_train\n",
    "        test_loss /= num_test\n",
    "\n",
    "        assert len(num_atc_res) == len(label_res)\n",
    "        assert len(num_res) == len(pred_res)\n",
    "\n",
    "        true_label, true_pred = [], []\n",
    "        for i in range(len(label_res)):\n",
    "            if num_res[i] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                true_pred.extend([pred_res[i]] * num_res[i])\n",
    "                true_label.extend(\n",
    "                    [1] * num_atc_res[i] + [0] * (num_res[i] - num_atc_res[i])\n",
    "                )\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(true_label, true_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        judge = [1 if sc > waterline else 0 for sc in true_pred]\n",
    "        f1 = f1_score(true_label, judge, average=\"macro\")\n",
    "        f2 = fbeta_score(true_label, judge, average=\"macro\", beta=2)\n",
    "        per = precision_score(true_label, judge, average=\"macro\")\n",
    "        rec = recall_score(true_label, judge, average=\"macro\")\n",
    "        acc = accuracy_score(true_label, judge)\n",
    "        mcc = matthews_corrcoef(true_label, judge)\n",
    "        fp_v, tp_v, _ = roc_curve(true_label, judge)\n",
    "        if len(tp_v) != 3 or len(fp_v) != 3:\n",
    "            logging.warn(\"Incorrect value for metrics.\")\n",
    "            continue\n",
    "\n",
    "        def cal_eer(fpr, tpr):\n",
    "            deta = 1\n",
    "            err = 0\n",
    "            for a, b in zip(fpr, tpr):\n",
    "                d = math.fabs((1 - a) - b)\n",
    "                if d < deta:\n",
    "                    deta = d\n",
    "                    err = a\n",
    "            return err\n",
    "\n",
    "        eer = cal_eer(fpr, tpr)\n",
    "\n",
    "        packet_per_frame = testD.size(2)\n",
    "        print(\n",
    "            f\"Epoch: {e:2d}, train loss: {train_loss:7.4f}, test loss: {test_loss:7.4f}, \"\n",
    "            f\"AUC: {roc_auc:7.4f}, F1: {f1:7.4f}, Percision: {per:7.4f}, Recall: {rec:7.4f}, F2: {f2:7.4f}, \"\n",
    "            f\"FPR: {fp_v[1]:7.4f}, TPR: {tp_v[1]:7.4f}, EER: {eer:7.4f}, MCC: {mcc:7.4f}, ACC: {acc:7.4f}, \"\n",
    "            f\"Test Time: {sum_test_time:7.4f} s, Test Speed: {(num_test*packet_per_frame)/sum_test_time:7.2f} PPS.\",\n",
    "            file=fout,\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "        # Save the distribution of scores\n",
    "        benign_score = [\n",
    "            x[1] for x in filter(lambda x: not x[0], list(zip(true_label, true_pred)))\n",
    "        ]\n",
    "        attack_score = [\n",
    "            x[1] for x in filter(lambda x: x[0], list(zip(true_label, true_pred)))\n",
    "        ]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10 * 0.618), constrained_layout=True)\n",
    "        ax = fig.subplots(1, 1)\n",
    "\n",
    "        ax.hist(\n",
    "            benign_score,\n",
    "            1000,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            cumulative=True,\n",
    "            label=\"Benign\",\n",
    "            color=\"royalblue\",\n",
    "        )\n",
    "        ax.hist(\n",
    "            attack_score,\n",
    "            1000,\n",
    "            density=True,\n",
    "            histtype=\"step\",\n",
    "            cumulative=True,\n",
    "            label=\"Attack\",\n",
    "            color=\"firebrick\",\n",
    "        )\n",
    "        ax.vlines(\n",
    "            waterline, 0, 1.05, lw=1, color=\"grey\", linestyles=\"--\", label=\"Waterline\"\n",
    "        )\n",
    "        ax.legend(loc=\"right\")\n",
    "        ax.set_xlabel(\"Score\")\n",
    "        ax.set_ylabel(\"CDF\")\n",
    "        ax.set_title(f\"Detection Accuracy: {data_tag}\")\n",
    "\n",
    "        save_addr = f\"{fig_path}/{data_tag}_result.png\"\n",
    "        fig.savefig(save_addr, dpi=600, format=\"png\")\n",
    "        plt.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8505a60-699f-4b24-8fe8-31f1c56a498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "import json\n",
    "import os\n",
    "import multiprocessing\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "seed = 777\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def train_task(\n",
    "    data_tag,\n",
    "    data_path,\n",
    "    log_path,\n",
    "    fig_path,\n",
    "    gpu_id,\n",
    "    waterline,\n",
    "    train_test_params,\n",
    "    data_set_params,\n",
    "):\n",
    "\n",
    "    dataset = load_data(\n",
    "        data_tag, data_path, shuffle_seed=seed, fig_path=fig_path, **data_set_params\n",
    "    )\n",
    "    train_test(\n",
    "        data_tag, log_path, fig_path, *dataset, gpu_id, waterline, **train_test_params\n",
    "    )\n",
    "\n",
    "\n",
    "def validate_json_config(jin: Dict) -> bool:\n",
    "    key_list = [\n",
    "        \"data_path\",\n",
    "        \"log_path\",\n",
    "        \"fig_path\",\n",
    "        \"data_tag\",\n",
    "        \"gpu_enable\",\n",
    "        \"data_construct_param\",\n",
    "        \"train_param\",\n",
    "    ]\n",
    "    for k in key_list:\n",
    "        if k not in jin:\n",
    "            print(f\"Key {k} is missed in configuration.\")\n",
    "            return False\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(jin[\"log_path\"]):\n",
    "            os.makedirs(jin[\"log_path\"])\n",
    "        if not os.path.exists(jin[\"fig_path\"]):\n",
    "            os.makedirs(jin[\"fig_path\"])\n",
    "        if \"model_save\" in jin and not os.path.exists(jin[\"model_save\"]):\n",
    "            os.makedirs(jin[\"model_save\"])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exceprtion in create folder\" + e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "def run():\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description='ExoSphere: detect DDoS at L2.')\n",
    "    # parser.add_argument('-c', '--config', type=str, default='./config.json', help='Configuration file.')\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    config_file = \"./config/config_amplification.json\"\n",
    "    with open(config_file, \"r\") as f:\n",
    "        jin = json.load(f)\n",
    "    if not validate_json_config(jin):\n",
    "        exit(-1)\n",
    "\n",
    "    # pve = []\n",
    "    for i, tag in enumerate(jin[\"data_tag\"].keys()):\n",
    "        _gpu_id = (\n",
    "            jin[\"gpu_enable\"][i % len(jin[\"gpu_enable\"])]\n",
    "            if len(jin[\"gpu_enable\"]) != 0\n",
    "            else -1\n",
    "        )\n",
    "        train_task(\n",
    "            tag,\n",
    "            f\"{jin['data_path']}/{tag}.txt\",\n",
    "            f\"{jin['log_path']}/{tag}.log\",\n",
    "            jin[\"fig_path\"],\n",
    "            _gpu_id,\n",
    "            jin[\"data_tag\"][tag],\n",
    "            jin[\"train_param\"],\n",
    "            jin[\"data_construct_param\"],\n",
    "        )\n",
    "\n",
    "    # [p.start() for p in pve]\n",
    "    # [p.join() for p in pve]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15fa6d52-6a9c-4fd5-8d57-b68c69400bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m[2025-02-24 17:59:16 INFO]\u001b[0m Read dataset from ./dataset/amp_ntp.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/amp_ntp.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run()\n",
      "Cell \u001b[0;32mIn[11], line 70\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_tag\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     69\u001b[0m     _gpu_id \u001b[38;5;241m=\u001b[39m jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_enable\u001b[39m\u001b[38;5;124m'\u001b[39m][i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_enable\u001b[39m\u001b[38;5;124m'\u001b[39m])] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_enable\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 70\u001b[0m     train_task(\n\u001b[1;32m     71\u001b[0m         tag,\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m         jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfig_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     75\u001b[0m         _gpu_id,\n\u001b[1;32m     76\u001b[0m         jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_tag\u001b[39m\u001b[38;5;124m'\u001b[39m][tag],\n\u001b[1;32m     77\u001b[0m         jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_param\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m         jin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_construct_param\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m, in \u001b[0;36mtrain_task\u001b[0;34m(data_tag, data_path, log_path, fig_path, gpu_id, waterline, train_test_params, data_set_params)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_task\u001b[39m(data_tag, data_path, log_path, fig_path, gpu_id, waterline, \n\u001b[1;32m     28\u001b[0m                 train_test_params, data_set_params):\n\u001b[0;32m---> 30\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m load_data(data_tag, data_path, shuffle_seed\u001b[38;5;241m=\u001b[39mseed, fig_path\u001b[38;5;241m=\u001b[39mfig_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_set_params)\n\u001b[1;32m     31\u001b[0m     train_test(data_tag, log_path, fig_path, \u001b[38;5;241m*\u001b[39mdataset, gpu_id, waterline, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_test_params)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mtime_log.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m     39\u001b[0m     _time_str \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     41\u001b[0m     _time_stp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(string_decorator(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() Finished] cost time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_time_stp\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m_time_str\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_tag, data_target, shuffle_seed, fig_path, train_ratio, segment_len, time_base)\u001b[0m\n\u001b[1;32m     23\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRead dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m tim_vec, len_vec, label_vec \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m     27\u001b[0m         [a, b, c] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mfloat\u001b[39m, ll\u001b[38;5;241m.\u001b[39msplit()))\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/amp_ntp.txt'"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf3115c-c9f3-43cc-aaf0-54ff4cced647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
